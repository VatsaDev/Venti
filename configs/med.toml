n_embd = 512   
n_layer = 16
n_head = 4     # keep head_dim 128

batch_size = 48	      # this is global, each gpu gets 80//4=20
block_size = 4096     # 4k context length, increase later
eval_interval = 20   
grad_accum_steps = 10 # Global batch size of 800

lr = 3e-4            # Slightly lower LR for larger width/batch
min_lr = 3e-5
max_iters = 20000    # Adjusted for slightly more training steps
eval_iters = 20
warmup_iters = 500   # Longer warmup for 8k context stability

dropout = 0.0        # Keep at 0.0 for H100 throughput unless overfiting
weight_decay = 0.1   # Standard for modern LLMs

swa_ratio = 1        # Every layer is global
data_dir = "qwen_text"
tokenizer = "Qwen/Qwen3-1.7B"
