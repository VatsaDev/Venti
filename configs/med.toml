# ~100M param model optimized for H100
n_embd = 256   # Increased from 512 for better H100 utilization
n_layer = 5    # Standard "Base" depth for 100M class
n_head = 2     # Head dimension remains 128, which is GPU-friendly

batch_size = 40 # this is global, each gpu gets 80//4=20     # H100 can easily handle 32-64 even at 8k context
block_size = 8192    # 8k context length
eval_interval = 20   
grad_accum_steps = 10 # Global batch size of 128 (32 * 4). Better for stability.

lr = 6e-4            # Slightly lower LR for larger width/batch
min_lr = 6e-5
max_iters = 20000    # Adjusted for slightly more training steps
eval_iters = 20
warmup_iters = 500   # Longer warmup for 8k context stability

dropout = 0.0        # Keep at 0.0 for H100 throughput unless overfiting
weight_decay = 0.1   # Standard for modern LLMs

swa_ratio = 4        # Every 4th layer is global
data_dir = "qwen_text"
tokenizer = "Qwen/Qwen3-1.7B"
